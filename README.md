# RomanUrdu-Transliterator

A complete **Neural Machine Translation (NMT)** system that converts **Urdu text into Roman Urdu** using a **Bidirectional LSTM Encoder** and **LSTM Decoder** implemented in **PyTorch**. This project was developed as part of my **NLP course assignment** and includes preprocessing, model architecture, training pipeline, evaluation metrics, and an optional Streamlit deployment.

---

# ğŸ“Œ Project Overview

This project performs **character-level transliteration** from *Urdu script* â†’ *Roman Urdu* using a deep learning approach. Rather than using simple rule-based mappings, the model learns how Roman Urdu is formed from Urdu characters using a sequence-to-sequence architecture.

---

# ğŸ¯ Objectives

* Build a **Seq2Seq NMT system** using:

  * **BiLSTM Encoder** (2 layers)
  * **LSTM Decoder** (4 layers)
* Train on *low-resource poetic Urdu data*
* Compare different hyperparameters
* Evaluate using **BLEU**, **Perplexity**, and **CER**
* Deploy the final model using **Streamlit**

---

# ğŸ“‚ Dataset

Dataset used: **urdu_ghazals_rekhta**

* Contains Urdu text, transliteration, and Hindi.
* We extract **Urdu â†’ Roman Urdu pairs**.
* Additional preprocessing applied (normalization, diacritics removal).

Dataset link:
ğŸ‘‰ [https://github.com/amir9ume/urdu_ghazals_rekhta](https://github.com/amir9ume/urdu_ghazals_rekhta)

---

# ğŸ§¹ Preprocessing Steps

### âœ” Unicode normalization

### âœ” Diacritics removal

### âœ” Standardization of Alef/Yeh forms

### âœ” Custom Urdu â†’ Roman Urdu mapping rules

### âœ” Tokenization (character-level)

These steps are implemented inside `preprocessing/text_cleaning.py`.

---

# ğŸ§  Model Architecture

### **ğŸ”¹ Encoder â€“ BiLSTM**

* Learns bidirectional context of Urdu characters
* 2 layers
* Hidden size: 256/512
* Embedding size: 128/256/512

### **ğŸ”¹ Decoder â€“ LSTM**

* 4 layers
* Uses teacher forcing
* Predicts Roman Urdu characters
* Optional attention-like context (mean of encoder outputs)

### **ğŸ”¹ Seq2Seq Wrapper**

* Connects encoder and decoder
* Handles training loop token-by-token

---

# ğŸ‹ï¸ Training Pipeline

* Train/Val/Test split: **50% / 25% / 25%**
* Optimizer: **Adam**
* Loss: **CrossEntropyLoss**
* Batch sizes: 32 / 64 / 128
* Learning rates tested: 1e-3, 5e-4, 1e-4
* Teacher forcing ratio: 0.5

Training file is located at: `training/train.py`

---

# ğŸ“Š Evaluation Metrics

The following metrics were implemented:

### âœ” BLEU Score (main NMT metric)

### âœ” Perplexity (model confidence)

### âœ” CER â€“ Character Error Rate

### âœ” Levenshtein Distance

Sample evaluation implemented in: `evaluation/evaluate.py`

---

# ğŸŒ Deployment (Streamlit App)

A simple **Streamlit UI** is included to test the model:

* User enters Urdu text
* Model outputs Roman Urdu
* Deployed using `streamlit run deployment/app.py`


---

# ğŸ“ Project Structure

```
ğŸ“¦ Urdu-to-Roman-Urdu-NMT
â”œâ”€â”€ data/
â”‚   â””â”€â”€ (dataset files go here)
â”œâ”€â”€ preprocessing/
â”‚   â””â”€â”€ text_cleaning.py
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ encoder.py
â”‚   â”œâ”€â”€ decoder.py
â”‚   â””â”€â”€ seq2seq.py
â”œâ”€â”€ training/
â”‚   â””â”€â”€ train.py
â”œâ”€â”€ evaluation/
â”‚   â””â”€â”€ evaluate.py
â”œâ”€â”€ deployment/
â”‚   â””â”€â”€ app.py
â”œâ”€â”€ notebooks/
â”‚   â””â”€â”€ nlp-a1-22f-8781-22f-3606.ipynb
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

---


**Haider Abbas**
FAST NUCES â€” NLP Course Assignment
